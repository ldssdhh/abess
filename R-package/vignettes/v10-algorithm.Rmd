---
title: 'ABESS algorithm: details'
author: "Jin Zhu"
date: "7/22/2021"
output:
  html_document: 
    toc: yes
    keep_md: yes
    self_contained: no
  pdf_document:
    fig_caption: yes
    toc: yes
    toc_depth: 3
  word_document: 
    toc: yes
    keep_md: yes
editor_options: 
  markdown: 
    wrap: sentence
vignette: |
  %\VignetteEngine{knitr::rmarkdown} \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

# Introduction              
The ABESS algorithm employing "splicing" technique can exactly solve general best subset problem in a polynomial time. The aim of this page to provide a 
complete and coherent documentation for ABESS algorithm such that users can 
easily understand the ABESS algorithm and its variants, thereby facilitating 
the usage of `abess` software. 

#  linear regression             
## Sacrifices

Consider the $\ell_{0}$ constraint minimization problem,
$$
\min _{\boldsymbol{\beta}} \mathcal{L}_{n}(\beta), \quad \text { s.t }\|\boldsymbol{\beta}\|_{0} \leq \mathrm{s},
$$
where $\mathcal{L}_{n}(\boldsymbol \beta)=\frac{1}{2 n}\|y-X \beta\|_{2}^{2} .$ Without loss of generality, we consider $\|\boldsymbol{\beta}\|_{0}=\mathrm{s}$. Given any initial set $\mathcal{A} \subset \mathcal{S}=\{1,2, \ldots, p\}$ with cardinality $|\mathcal{A}|=s$,
denote $\mathcal{I}=\mathcal{A}^{\mathrm{c}}$ and compute
$$
\hat{\boldsymbol{\beta}}=\arg \min _{\boldsymbol{\beta}_{\mathcal{I}}=0} \mathcal{L}_{n}(\boldsymbol{\beta}).
$$
We call $\mathcal{A}$ and $\mathcal{I}$ as the active set and the inactive set, respectively.

Given the active set $\mathcal{A}$ and $\hat{\boldsymbol{\beta}}$, we can define the following two types of sacrifices:

1) Backward sacrifice: For any $j \in \mathcal{A}$, the magnitude of discarding variable $j$ is,
$$
\xi_{j}=\mathcal{L}_{n}\left(\hat{\boldsymbol{\beta}}^{\mathcal{A} \backslash\{j\}}\right)-\mathcal{L}_{n}\left(\hat{\boldsymbol{\beta}}^{\mathcal{A}}\right)=\frac{X_{j}^{\top} X_{j}}{2 n}\left(\hat{\boldsymbol\beta}_{j}\right)^{2},
$$
2) Forward sacrifice: For any $j \in \mathcal{I}$, the magnitude of adding variable $j$ is,
$$
\zeta_{j}=\mathcal{L}_{n}\left(\hat{\boldsymbol{\beta}^{\mathcal{A}}}\right)-\mathcal{L}_{n}\left(\hat{\boldsymbol{\beta}}^{\mathcal{A}}+\hat{t}^{\{j\}}\right)=\frac{X_{j}^{\top} X_{j}}{2 n}\left(\frac{\hat{\boldsymbol d}_{j}}{X_{j}^{\top} X_{j} / n}\right)^{2}.
$$
where $\hat{t}=\arg \min _{t} \mathcal{L}_{n}\left(\hat{\boldsymbol{\beta}}^{\mathcal{A}}+t^{\{j\}}\right), \hat{\boldsymbol d}_{j}=X_{j}^{\top}(y-X \hat{\boldsymbol{\beta}}) / n$
Intuitively, for $j \in \mathcal{A}$ (or $j \in \mathcal{I}$ ), a large $\xi_{j}$ (or $\zeta_{j}$) implies the $j$ th variable is potentially important.

## Algorithm

### Best-Subset Selection with a Given Support Size
 Unfortunately, it is noteworthy that these two sacrifices are incomparable because they have different sizes of support set. However, if we exchange some "irrelevant" variables in $\mathcal{A}$ and some "important" variables in $\mathcal{I}$, it may result in a higher-quality solution. This intuition motivates our splicing method. Specifically, given any splicing size $k \leq s$, define

$$
\mathcal{A}_{k}=\left\{j \in \mathcal{A}: \sum_{i \in \mathcal{A}} \mathrm{I}\left(\xi_{j} \geq \xi_{i}\right) \leq k\right\}
$$
to represent $k$ least relevant variables in $\mathcal{A}$ and
$$
\mathcal{I}_{k}=\left\{j \in \mathcal{I}: \sum_{i \in \mathcal{I}} \mid\left(\zeta_{j} \leq \zeta_{i}\right) \leq k\right\}
$$
to represent $k$ most relevant variables in $\mathcal{I} .$ Then, we splice $\mathcal{A}$ and $\mathcal{I}$ by exchanging $\mathcal{A}_{k}$ and $\mathcal{I}_{k}$ and obtain a new active set
$$
\tilde{\mathcal{A}}=\left(\mathcal{A} \backslash \mathcal{A}_{k}\right) \cup \mathcal{I}_{k}.
$$
Let $\tilde{\mathcal{I}}=\tilde{\mathcal{A}}^{c}, \tilde{\boldsymbol{\beta}}=\arg \min _{\boldsymbol{\beta}_{\overline{\mathcal{I}}}=0} \mathcal{L}_{n}(\boldsymbol{\beta})$, and $\tau_{s}>0$ be a threshold. If $\tau_{s}<$
$\mathcal{L}_{n}(\hat{\boldsymbol\beta})-\mathcal{L}_{n}(\tilde{\boldsymbol\beta})$, then $\tilde{A}$ is preferable to $\mathcal{A} .$ The active set can be updated
iteratively until the loss function cannot be improved by splicing. Once the algorithm recovers the true active set, we may splice some irrelevant variables, and then the loss function may decrease slightly. The threshold $\tau_{s}$ can reduce this unnecessary calculation. Typically, $\tau_{s}$ is relatively small, e.g. $\tau_{s}=0.01 s \log (p) \log (\log n) / n.$


#### Algorithm 1: BESS.Fix(s): Best-Subset Selection with a given support size s.

1) Input: $X, y$, a positive integer $k_{\max }$, and a threshold $\tau_{s}$.
2) Initialize $\mathcal{A}^{0}=\left\{j: \sum_{i=1}^{p} \mathrm{I}\left(\left|\frac{X_{j}^{\top} y}{\sqrt{X_{j}^{\top} X_{j}}}\right| \leq \left| \frac{X_{i}^{\top} y}{\sqrt{X_{i}^{\top} X_{i}}}\right| \leq \mathrm{s}\right\}, \mathcal{I}^{0}=\left(\mathcal{A}^{0}\right)^{c}\right.$,
and $\left(\boldsymbol\beta^{0}, d^{0}\right):$

\begin{align*}
    &\boldsymbol{\beta}_{\mathcal{I}^{0}}^{0}=0,\\
    &d_{\mathcal{A}^{0}}^{0}=0,\\
&\boldsymbol{\beta}_{\mathcal{A}^{0}}^{0}=\left(\boldsymbol{X}_{\mathcal{A}^{0}}^{\top} \boldsymbol{X}_{\mathcal{A}^{0}}\right)^{-1} \boldsymbol{X}_{\mathcal{A}^{0}}^{\top} \boldsymbol{y},\\
&d_{\mathcal{I}^{0}}^{0}=X_{\mathcal{I}^{0}}^{\top}\left(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta}^{0}\right).
\end{align*}

3) For $m=0,1, \ldots$, do 

   $$\left(\boldsymbol{\beta}^{m+1}, \boldsymbol{d}^{m+1}, \mathcal{A}^{m+1}, \mathcal{I}^{m+1}\right)= \text{Splicing} \left(\boldsymbol{\beta}^{m}, \boldsymbol{d}^{m}, \mathcal{A}^{m}, \mathcal{I}^{m}, k_{\max }, \tau_{s}\right).$$

    If $\left(\mathcal{A}^{m+1}, \mathcal{I}^{m+1}\right)=\left(\mathcal{A}^{m}, \mathcal{I}^{m}\right)$, then stop

    End For

4) Output $(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{d}}, \hat{\mathcal{A}}, \hat{\mathcal{I}})=\left(\boldsymbol{\beta}^{m+1}, \boldsymbol{d}^{m+1} \mathcal{A}^{m+1}, \mathcal{I}^{m+1}\right).$


#### Algorithm 2: Splicing $\left(\boldsymbol\beta, d, \mathcal{A}, \mathcal{I}, k_{\max }, \tau_{s}\right)$

1) Input: $\boldsymbol{\beta}, \boldsymbol{d}, \mathcal{A}, \mathcal{I}, k_{\max }$, and $\tau_{\mathrm{s}} .$
2) Initialize $L_{0}=L=\frac{1}{2 n}\|y-X \beta\|_{2}^{2}$, and set $\xi_{j}=\frac{X_{j}^{\top} X_{j}}{2 n}\left(\beta_{j}\right)^{2}, \zeta_{j}=\frac{X_{j}^{\top} X_{j}}{2 n}\left(\frac{d_{j}}{X_{j}^{\top} X_{j} / n}\right)^{2}, j=1, \ldots, p.$
3) For $k=1,2, \ldots, k_{\max }$, do

     $$\mathcal{A}_{k}=\left\{j \in \mathcal{A}: \sum_{i \in \mathcal{A}} \mathrm{I}\left(\xi_{j} \geq \xi_{i}\right) \leq k\right\}$$

    $$\mathcal{I}_{k}=\left\{j \in \mathcal{I}: \sum_{i \in \mathcal{I}} \mathrm{I}\left(\zeta_{j} \leq \zeta_{i}\right) \leq k\right\}$$

    Let $\tilde{\mathcal{A}}_{k}=\left(\mathcal{A} \backslash \mathcal{A}_{k}\right) \cup \mathcal{I}_{k}, \tilde{\mathcal{I}}_{k}=\left(\mathcal{I} \backslash \mathcal{I}_{k}\right) \cup \mathcal{A}_{k}$ and solve

    
    $$\tilde{\boldsymbol{\beta}}_{{\mathcal{A}}_{k}}=\left(\boldsymbol{X}_{\mathcal{A}_{k}}^{\top} \boldsymbol{X}_{{\mathcal{A}}_{k}}\right)^{-1} \boldsymbol{X}_{{\mathcal{A}_{k}}}^{\top} y, \quad \tilde{\boldsymbol{\beta}}_{{\mathcal{I}}_{k}}=0$$

    $$\tilde{\boldsymbol d}_{\mathcal{I}^k}=X_{\mathcal{I}^k}^{\top}(y-X \tilde{\beta}) / n,\quad \tilde{\boldsymbol d}_{\mathcal{A}^k} = 0.$$ 
    
    Compute $\mathcal{L}_{n}(\tilde{\boldsymbol\beta})=\frac{1}{2 n}\|y-X \tilde{\boldsymbol\beta}\|_{2}^{2}.$

    If $L>\mathcal{L}_{n}(\tilde{\boldsymbol\beta})$, then

    $$(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{d}}, \hat{\mathcal{A}}, \hat{\mathcal{I}})=\left(\tilde{\boldsymbol{\beta}}, \tilde{\boldsymbol{d}}, \tilde{\mathcal{A}}_{k}, \tilde{\mathcal{I}}_{k}\right)$$

    $$L=\mathcal{L}_{n}(\tilde{\boldsymbol\beta}).$$

    End for

4) If $L_{0}-L<\tau_{s}$, then $(\hat{\boldsymbol\beta}, \hat{d}, \hat{A}, \hat{I})=(\boldsymbol\beta, d, \mathcal{A}, \mathcal{I}).$
5) Output $(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{d}}, \hat{\mathcal{A}}, \hat{\mathcal{I}})$.


<!-- In practice, the support size is usually unknown. We use a data-driven procedure to determine s. Information criteria such as highdimensional BIC (HBIC) (13) and extended BIC (EBIC) (14) are commonly used for this purpose. Specifically, HBIC (13) can be applied to select the tuning parameter in penalized likelihood estimation. To recover the support size $s$ for the best-subset selection, we introduce a criterion that is a special case of HBIC (13). While HBIC aims to tune the parameter for a nonconvex penalized regression, our proposal is used to determine the size of best subset. For any active set $\mathcal{A}$, define an $\mathrm{SIC}$ as follows: -->
<!-- $$ -->
<!-- \operatorname{SIC}(\mathcal{A})=n \log \mathcal{L}_{\mathcal{A}}+|\mathcal{A}| \log (p) \log \log n, -->
<!-- $$ -->
<!-- where $\mathcal{L}_{\mathcal{A}}=\min _{\beta_{\mathcal{I}}=0} \mathcal{L}_{n}(\beta), \mathcal{I}=(\mathcal{A})^{c}$. To identify the true model, the -->
<!-- model complexity penalty is $\log p$ and the slow diverging rate $\log \log n$ is set to prevent underfitting. Theorem 4 states that the following ABESS algorithm selects the true support size via SIC. -->

<!-- Let $s_{\max }$ be the maximum support size. Theorem 4 suggests $s_{\max }=o\left(\frac{n}{\log p}\right)$ as the maximum possible recovery size. Typically, we set $s_{\max }=\left[\frac{n}{\log p \log \log n}\right]$ -->
<!-- where $[x]$ denotes the integer part of $x$. -->

### Determining the Best Support Size with SIC

In practice, the support size is usually unknown. We use a data-driven procedure to determine s. For any active set $\mathcal{A}$, define an $\mathrm{SIC}$ as follows:
$$
\operatorname{SIC}(\mathcal{A})=n \log \mathcal{L}_{\mathcal{A}}+|\mathcal{A}| \log (p) \log \log n,
$$
where $\mathcal{L}_{\mathcal{A}}=\min _{\beta_{\mathcal{I}}=0} \mathcal{L}_{n}(\beta), \mathcal{I}=(\mathcal{A})^{c}$. To identify the true model, the
model complexity penalty is $\log p$ and the slow diverging rate $\log \log n$ is set to prevent underfitting. Theorem 4 states that the following ABESS algorithm selects the true support size via SIC.

Let $s_{\max }$ be the maximum support size. We suggest $s_{\max }=o\left(\frac{n}{\log p}\right)$ as the maximum possible recovery size. Typically, we set $s_{\max }=\left[\frac{n}{\log p \log \log n}\right]$
where $[x]$ denotes the integer part of $x$.


#### Algorithm 3: ABESS.

1) Input: $X, y$, and the maximum support size $s_{\max } .$

2) For $s=1,2, \ldots, s_{\max }$, do

    $$\left(\hat{\boldsymbol{\beta}}_{s}, \hat{\boldsymbol{d}}_{s}, \hat{\mathcal{A}}_{s}, \hat{\mathcal{I}}_{s}\right)= \text{BESS.Fixed}(s).$$

    End for

3) Compute the minimum of SIC:

    $$s_{\min }=\arg \min _{s} \operatorname{SIC}\left(\hat{\mathcal{A}}_{s}\right).$$

4) Output $\left(\hat{\boldsymbol{\beta}}_{s_{\operatorname{min}}}, \hat{\boldsymbol{d}}_{s_{\min }}, \hat{A}_{s_{\min }}, \hat{\mathcal{I}}_{s_{\min }}\right) .$

# Group linear model

## Sacrifices

Consider the $\ell_{0,2}$ constraint minimization problem with $n$ samples and $J$ non-overlapping groups,
$$
\min _{\boldsymbol{{\boldsymbol\beta}}} \mathcal{L}({\boldsymbol\beta}), \quad \text { s.t }\|{{\boldsymbol\beta}}\|_{0,2} \leq \mathrm{T}.
$$
where $\mathcal{L}({\boldsymbol\beta})$ is the negative log-likelihood function and support size $\mathrm{T}$ is a positive number. Without loss of generality, we consider $\|\boldsymbol{{\boldsymbol\beta}}\|_{0,2}=\mathrm{T}$. Given any group subset $\mathcal{A} \subset \mathcal{S}=\{1,2, \ldots, J\}$ with cardinality $|\mathcal{A}|=\mathrm{T}$,
denote $\mathcal{I}=\mathcal{A}^{\mathrm{c}}$ and compute
$$
\hat{{{\boldsymbol\beta}}}=\arg \min _{{{\boldsymbol\beta}}_{\mathcal{I}}=0} \mathcal{L}({{\boldsymbol\beta}}).
$$
We call $\mathcal{A}$ and $\mathcal{I}$ as the selected group subset and the unselected group subset, respectively.
Denote $g_{G_j} = [{\nabla} \mathcal{L}({\boldsymbol\beta})]_{G_j} $ as the $j$th group gradient of $({\boldsymbol\beta})$ and $h_{G_j} = [{\nabla}^2 \mathcal{L}({\boldsymbol\beta})]_{G_j} $ as the $j$th group diagonal sub-matrix of hessian matrix of $\mathcal{L}({\boldsymbol\beta})$. Let dual variable $d_{G_j} = -g_{G_j}$ and $\Psi_{G_j} =  (h_{G_j})^{\frac{1}{2}}$.

Given the selected group subset $\mathcal{A}$ and $\hat{\boldsymbol{{\boldsymbol\beta}}}$, we can define the following two types of sacrifices:

1) Backward sacrifice: For any $j \in \mathcal{A}$, the magnitude of discarding group $j$ is,
$$
\xi_j = \mathcal{L}({\boldsymbol\beta}^{\mathcal{A}^k\backslash j})-\mathcal{L}({\boldsymbol\beta}^k)=\frac{1}{2}({\boldsymbol\beta}^k_{G_j})^k h^k_{G_j}{\boldsymbol\beta}^k_{G_j} = \frac{1}{2}\|\bar{{\boldsymbol\beta}}_{G_j}^k\|_2^2,
$$
where ${\boldsymbol\beta}^{\mathcal{A}^k\backslash j}$ is the estimator assigning the $j$th group of ${\boldsymbol\beta}^k$ to be zero and $\bar {\boldsymbol\beta}_{G_j}^k=\Psi^k_{G_j} {\boldsymbol\beta}_{G_j}^k$.
2) Forward sacrifice: For any $j \in \mathcal{I}$, the magnitude of adding variable $j$ is,
$$
\zeta_{j}=\mathcal{L}({\boldsymbol\beta}^k)-\mathcal{L}({\boldsymbol\beta}^k+t_j^k)=\frac{1}{2}(d_{G_j}^k)^\top (h^k_{G_j})^{-1} d^k_{G_j}= \frac{1}{2}\|\bar{d}^k_{G_j}\|_2^2,
$$
where $t^k_j = \arg\min\limits_{t_{G_j} \neq 0}L({\boldsymbol\beta}^k+t)$ and $\bar d_{G_j}^k = (\Psi^k_{G_j})^{-1} d^k_{G_j}$.

Intuitively, for $j \in \mathcal{A}$ (or $j \in \mathcal{I}$ ), a large $\xi_{j}$ (or $\zeta_{j}$) implies the $j$ th group is potentially important.

We show four useful examples in the following.

#### Case 1 : Group linear model.

\noindent In group linear model, the loss function is
\begin{equation*}
\mathcal{L}({\boldsymbol\beta}) = \frac{1}{2}\|y-X{\boldsymbol\beta}\|_2^2.
\end{equation*}
We have
\begin{equation*}
d_{G_j} = X_{G_j}^\top(y-X{\boldsymbol\beta})/n,\ \Psi_{G_j} = (X_{G_j}^\top X_{G_j}/n)^{\frac{1}{2}}, \ j=1,\ldots,J.
\end{equation*}

Under the assumption of orthonormalization, that is $X_{G_j}^\top X_{G_j}/n = I_{p_j}, j=1,\ldots, J$. we have $\Psi_{G_j}=I_{p_j}$. Thus for linear regression model, we do not need to update $\Psi$ during iteration procedures.

#### Case 2 : Group logistic model.

\noindent Given the data $\{(X_i, y_i)\}_{i=1}^{n}$ with $y_i \in \{0, 1\}, X_i \in \mathbb{R}^p$, and denote $X_i = (X_{i, G_1}^\top,\ldots, X_{i, G_J}^\top)^\top$.
Consider the logistic model $\log\{\pi/(1-\pi)\} = {\boldsymbol\beta}_0 +  x^\top{\boldsymbol\beta}$ with $x \in \mathbb{R}^p$ and $\pi = P(y=1|x)$.

\noindent Thus the negative log-likelihood function is
\begin{equation*}
  \mathcal{L}({\boldsymbol\beta}_0, {\boldsymbol\beta}) =  \sum_{i=1}^n  \{\log(1+\exp({\boldsymbol\beta}_0+X_i^\top {\boldsymbol\beta}))-y_i ({\boldsymbol\beta}_0+X_i^\top {\boldsymbol\beta})\}.
\end{equation*}

\noindent  We have

\begin{equation*}
d_{G_j} = X_{G_j}^\top(y-\pi),\ \Psi_{G_j} = (X_{G_j}^\top W X_{G_j})^{\frac{1}{2}}, \ j=1,\ldots,J,
\end{equation*}
where $\pi = (\pi_1,\ldots,\pi_n)$ with $\pi_i = \exp(X_i^\top {\boldsymbol\beta})/(1+\exp(X_i^\top {\boldsymbol\beta}))$, and $W$ is a diagonal matrix with $i$th diagonal entry equal to $\pi_i(1-\pi_i)$.

#### Case 3 : Group poisson model.

\noindent Given the data $\{(X_i, y_i)\}_{i=1}^{n}$ with $y_i \in \mathbb{N}, X_i \in \mathbb{R}^p$, and denote $X_i = (X_{i, G_1}^\top,\ldots, X_{i, G_J}^\top)^\top$.
Consider the poisson model $\log(\mathbb{E}(y|x)) = {\boldsymbol\beta}_0 + x^\top {\boldsymbol\beta}$ with $x \in \mathbb{R}^p$.

\noindent Thus the negative log-likelihood function is
\begin{equation*}
  \mathcal{L}({\boldsymbol\beta}_0, {\boldsymbol\beta}) =  \sum_{i=1}^n  \{\exp({\boldsymbol\beta}_0+X_i^\top {\boldsymbol\beta})+\log(y_i !)-y_i ({\boldsymbol\beta}_0+X_i^\top {\boldsymbol\beta})\}.
\end{equation*}

\noindent  We have

\begin{equation*}
d_{G_j} = X_{G_j}^\top(y-\eta),\ \Psi_{G_j} = (X_{G_j}^\top W X_{G_j})^{\frac{1}{2}}, \ j=1,\ldots,J,
\end{equation*}
where $\eta = (\eta_1,\ldots,\eta_n)$ with $\eta_i = \exp({\boldsymbol\beta}_0+X_i^\top{\boldsymbol\beta})$, and $W$ is a diagonal matrix with $i$th diagonal entry equal to $\eta_i$.\\

#### Case 4 : Group Cox proportional hazard model.

\noindent Given the survival data $\{(T_i, \delta_i, x_i)\}_{i=1}^n$ with observation of survival time $T_i$ an censoring indicator $\delta_i$.
Consider the Cox proportional hazard model $\lambda(x|t) = \lambda_0(t) \exp(x^\top {\boldsymbol\beta})$ with a baseline hazard $\lambda_0(t)$ and $x \in \mathbb{R}^p$. By the method of partial likelihood,
we can write the negative log-likelihood function as
\begin{equation*}
  \mathcal{L}({\boldsymbol\beta}) =  \log\{\sum_{i':T_{i'} \geqslant T_i} \exp(X_i^\top{\boldsymbol\beta})\}-\sum_{i:\delta_i = 1} X_i^\top {\boldsymbol\beta}.
\end{equation*}

\noindent We have

\begin{align*}
  &d_{G_j} = \sum_{i:\delta_i=1} (X_{i, G_j} - \sum_{i':T_{i'} > T_i} X_{i', G_j} \omega_{i, i'}),\\
  &\Psi_{G_j}=\{\sum_{i:\delta_i=1} (\{\sum_{i':T_{i'} > T_i} \omega_{i, i'} X_{i',G_j}\}\{\sum_{i':T_{i'} > T_i} \omega_{i, i'} X_{i',G_j}\}^\top-\sum_{i':T_{i'} > T_i} \omega_{i, i'} X_{i',G_j} X_{i', G_j}^\top)\}^{\frac{1}{2}},
\end{align*}
where $\omega_{i, i'} = \exp(X_{i'}^\top{\boldsymbol\beta})/\sum_{i':T_{i'} > T_i} \exp(X_{i'}^\top {\boldsymbol\beta})$.

## Algorithm

### Best Group Subset Selection with a determined support size
Motivated by the definition of sacrifices, we can extract the "irrelevant" groups in $\mathcal{A}$ and the "important" groups in $\mathcal{I}$, respectively, and then exchange them to get a high-quality solution.

Given any exchange subset size $C \leq C_{max}$, define the exchanged group subset as

$$
\mathcal{S}_{C,1}=\left\{j \in \mathcal{A}: \sum_{i \in \mathcal{A}} \mathrm{I}\left(\frac{1}{p_j}\xi_{j} \geq \frac{1}{p_i}\xi_{i}\right) \leq C\right\}
$$
and
$$
\mathcal{S}_{C,2}=\left\{j \in \mathcal{I}: \sum_{i \in \mathcal{I}} I\left(\frac{1}{p_j}\zeta_{j} \leq \frac{1}{p_i}\zeta_{i}\right) \leq C\right\},
$$
where $p_j$ is the number of variables in $j$th group.

From the definition of sacrifices, $\mathcal{S}_{C,1}\ (\mathcal{S}_{C,2})$ can be interpreted as the groups in $\mathcal{A}\ (\mathcal{I})$ with $C$ smallest (largest) contributions to the loss function. Then, we splice $\mathcal{A}$ and $\mathcal{I}$ by exchanging $\mathcal{S}_{C,1}$ and $\mathcal{S}_{C,2}$ and obtain a novel selected group subset
$$
\tilde{\mathcal{A}}=\left(\mathcal{A} \backslash \mathcal{S}_{C,1}\right) \cup \mathcal{S}_{C,2}.
$$
Let $\tilde{\mathcal{I}}=\tilde{\mathcal{A}}^{c}, \tilde{\boldsymbol{{\boldsymbol\beta}}}=\arg \min _{\boldsymbol{{\boldsymbol\beta}}_{\overline{\mathcal{I}}}=0} \mathcal{L}(\boldsymbol{{\boldsymbol\beta}})$, and $\pi_{T}>0$ be a threshold to eliminate unnecessary iterations. 
<!-- If $\tau_{s}<$ -->
<!-- $\mathcal{L}_{n}(\hat{{\boldsymbol\beta}})-\mathcal{L}_{n}(\tilde{{\boldsymbol\beta}})$, then $\tilde{A}$ is preferable to $\mathcal{A} .$ The active set can be updated -->
<!-- iteratively until the loss function cannot be improved by splicing. Once the algorithm recovers the true active set, we may splice some irrelevant variables, and then the loss function may decrease slightly. The threshold $\tau_{s}$ can reduce this unnecessary calculation. Typically, $\tau_{s}$ is relatively small, e.g. $\tau_{s}=0.01 s \log (p) \log (\log n) / n$ -->

We summarize the group-splicing algorithm as follows:

#### Algorithm 1: Group-Splicing.

1) Input: $X,\ y,\ \{G_j\}_{j=1}^J,\ T, \ \mathcal{A}^0,\ \pi_T, \ C_{\max}$.
2) Initialize $\mathcal{A}^{0}=\left\{j: \sum_{i=1}^{J} \mathrm{I}\left( g_{G_j} \leq g_{G_i}\right) \leq \mathrm{T}\right\}$ with ${\boldsymbol\beta} = \boldsymbol{0}$, 
$\mathcal{I}^{0}=\left(\mathcal{A}^{0}\right)^{c}$,
and $\left({\boldsymbol\beta}^{0}, d^{0}\right):$
\begin{align*}
    &{{\boldsymbol\beta}}_{\mathcal{A}^{0}}^{0}=[\arg \min _{{{\boldsymbol\beta}}_{\mathcal{I}^{0}}=0} \mathcal{L}({{\boldsymbol\beta}})]_{\mathcal{A}^{0}},\ {{\boldsymbol\beta}}_{\mathcal{I}^{0}}^{0}=0,\\
    &d_{\mathcal{I}^{0}}^{0}=[\nabla \mathcal{L}({\boldsymbol\beta}^0)]_{\mathcal{I}^0},\ d_{\mathcal{A}^{0}}^{0}=0.\\
    \end{align*}

3) For $k=0,1, \ldots$, do 

    Compute $L=\mathcal{L}({\boldsymbol\beta}^k)$ and update $\mathcal{S}_1^k, \mathcal{S}_2^k$
\begin{align*}
  &\mathcal{S}_1^k = \{j \in \mathcal{A}^k: \sum\limits_{i\in \mathcal{A}^k} I(\frac{1}{p_j}\|{\bar {\boldsymbol\beta}_{G_j}^k}\|_2^2 \geq \frac{1}{p_i}\|{\bar {\boldsymbol\beta}_{G_i}^k}\|_2^2) \leq C_{\max}\},\\
  &\mathcal{S}_2^k = \{j \in \mathcal{I}^k: \sum\limits_{i\in \mathcal{I}^k} I(\frac{1}{p_j}\|{\bar d_{G_j}^k}\|_2^2 \leq \frac{1}{p_i}\|{\bar d_{G_i}^k}\|_2^2) \leq C_{\max}\}.
\end{align*}

3) For $C=C_{\max}, \ldots, 1$, do 

    Let $\tilde{\mathcal{A}}^k_C=(\mathcal{A}^k\backslash \mathcal{S}_1^k)\cup \mathcal{S}_2^k\ \text{and}\ \tilde{\mathcal{I}}^k_C = (\mathcal{I}^k\backslash \mathcal{S}_2^k)\cup \mathcal{S}_1^k$.
    
    Update primal variable $\tilde{{\boldsymbol\beta}}$ and dual variable $\tilde{d}$
\begin{align*}
  \tilde{\boldsymbol\beta}=\arg \min _{{{\boldsymbol\beta}}_{\tilde{\mathcal{I}}^k_C}=0} \mathcal{L}({{\boldsymbol\beta}}),\ \tilde d = \nabla \mathcal{L}(\tilde{\boldsymbol\beta}).
\end{align*}

    Compute $\tilde L = \mathcal{L}(\tilde {\boldsymbol\beta})$.
    
    If $L-\tilde L < \pi_T$, 
        Denote $(\tilde{\mathcal{A}}^k_C, \tilde{\mathcal{I}}^k_C, \tilde {\boldsymbol\beta} , \tilde d )$ as $(\mathcal{A}^{k+1}, \mathcal{I}^{k+1}, {\boldsymbol\beta}^{k+1}, d^{k+1})$ and break.
        
      Else, Update $\mathcal{S}_1^k \text{and} \mathcal{S}_2^k$
\begin{align*}
  &\mathcal{S}_1^k = \mathcal{S}_1^k\backslash \arg\max\limits_{i \in \mathcal{S}_1^k} \{\frac{1}{p_i}\|{\bar {\boldsymbol\beta}_{G_i}^k}\|_2^2\},\\
  &\mathcal{S}_2^k = \mathcal{S}_2^k\backslash \arg\min\limits_{i \in \mathcal{S}_2^k} \{\frac{1}{p_i}\|{\bar d_{G_i}^k}\|_2^2\}.
\end{align*}
    
    End For

    If $\left(\mathcal{A}^{k+1}, \mathcal{I}^{k+1}\right)=\left(\mathcal{A}^{k}, \mathcal{I}^{k}\right)$, then stop.

    End for

4) Output $(\hat{\boldsymbol{{\boldsymbol\beta}}}, \hat{\boldsymbol{d}}, \hat{\mathcal{A}}, \hat{\mathcal{I}})=\left(\boldsymbol{{\boldsymbol\beta}}^{m+1}, \boldsymbol{d}^{m+1} \mathcal{A}^{m+1}, \mathcal{I}^{m+1}\right).$


### Determining the best support size with information criterion

Practically, the optimal support size is usually unknown. Thus, we use a data-driven procedure to determine $\mathrm{T}$. Due to the computational burden of cross validation, we prefer information criterion to conduct the selection procedure. 
For any selected group subset $\mathcal{A}$, define an group information criterion(GIC) as follows:
$$
\operatorname{GIC}(\mathcal{A})=n \log \mathcal{L}_{\mathcal{A}}+|\mathcal{A}| \log J \log \log n,
$$
where $\mathcal{L}_{\mathcal{A}}=\min _{{\boldsymbol\beta}_{\mathcal{I}}=0} \mathcal{L}_{n}({\boldsymbol\beta}), \mathcal{I}=(\mathcal{A})^{c}$. To identify the true model, the
model complexity penalty is $\log J$ and the slow diverging rate $\log \log n$ is set to prevent underfitting. Besides, we define the Bayesian group information criterion (BGIC) as follows:
$$
\operatorname{BGIC}(\mathcal{A})=n \log \mathcal{L}_{\mathcal{A}}+|\mathcal{A}| (\gamma \log J +\log n),
$$
where $\gamma$ is a pre-determined positive constant, controlling the diverging rate of group numbers $J$.

A natural idea to determine the optimal support size is regarding $\mathrm{T}$ as a tuning parameter, and running GSplicing algorithm over a sequence about $\mathrm{T}$. Next, combined with aforementioned information criterion, we can obtain an optimal support size.
Let $T_{\max }$ be the maximum support size. We suggest $T_{\max }=o\left(\frac{n}{p_{\max}\log J}\right)$ where $p_{\max} = \max_{j\in \mathcal{S}} p_j$. 


We summarize the sequential group-splicing algorithm with GIC as follows:

#### Algorithm 2: Sequential Group-Splicing (SGSplicing).

1) Input: $X,\ y,\ \{G_j\}_{j=1}^J,\ T_{\max}, \ \pi_T, \ C_{\max}.$

2) For $T=1,2, \ldots, T_{\max }$, do

    $$\left(\hat{\boldsymbol{{\boldsymbol\beta}}}_{T}, \hat{\boldsymbol{d}}_{T}, \hat{\mathcal{A}}_{T}, \hat{\mathcal{I}}_{T}\right)=\text{GSplicing}(X, y, \{G_j\}_{j=1}^J, T,  \mathcal{A}^0_T, \pi_T, C_{\max}).$$

    End for

3) Compute the minimum of GIC:

    $$T_{\min }=\arg \min _{T} \operatorname{GIC}\left(\hat{\mathcal{A}}_{T}\right).$$

4) Output $\left(\hat{\boldsymbol{{\boldsymbol\beta}}}_{T_{\operatorname{min}}}, \hat{\boldsymbol{d}}_{T_{\min }}, \hat{A}_{T_{\min }}, \hat{\mathcal{I}}_{T_{\min }}\right) .$

# Nuisance selection                

# Principal component analysis            
(By Junhao Huang)